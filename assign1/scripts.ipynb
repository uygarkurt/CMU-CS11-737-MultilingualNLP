{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "ea8f8077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['John', 'went', 'up']\n",
      "['PROPN', 'VERB', 'ADP']\n"
     ]
    }
   ],
   "source": [
    "from model import BiLSTMPOSTagger\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "import torch\n",
    "import json\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "vocab_text = torch.load(f\"saved_models/en-vocabtext.pth\")\n",
    "vocab_tag = torch.load(f\"saved_models/en-vocabtag.pth\")\n",
    "params = json.load(open(\"config.json\"))\n",
    "\n",
    "model = BiLSTMPOSTagger(\n",
    "        input_dim=len(vocab_text),\n",
    "        embedding_dim=params[\"embedding_dim\"],\n",
    "        hidden_dim=params[\"hidden_dim\"],\n",
    "        output_dim=len(vocab_tag),\n",
    "        n_layers=params[\"n_layers\"],\n",
    "        bidirectional=params[\"bidirectional\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        pad_idx=vocab_text['<PAD>'],\n",
    "    ).to(device)\n",
    "\n",
    "model.load_state_dict(torch.load(f\"saved_models/en-model.pt\", map_location=device))\n",
    "\n",
    "#vocab_tag.get_stoi() returns dict of label to index mappings. Reverse it here.\n",
    "idx_tag = {v: k for k, v in vocab_tag.get_stoi().items()}\n",
    "\n",
    "#transform the text and tags into their indicies from the vocabulary. Return them as tensors. That's what our\n",
    "#collate_fn does here in general.\n",
    "def transform_text(x):\n",
    "    return [vocab_text[token] for token in x]\n",
    "def transform_tag(x):\n",
    "    return [vocab_tag[tag] for tag in x]\n",
    "\n",
    "def collate_batch(batch):\n",
    "    tag_list, text_list = [], []\n",
    "    for (line, label) in batch:\n",
    "        text_list.append(torch.tensor(transform_text(line), device=device))\n",
    "        tag_list.append(torch.tensor(transform_tag(label), device=device))\n",
    "    return (\n",
    "        pad_sequence(text_list, padding_value=vocab_text['<PAD>']),\n",
    "        pad_sequence(tag_list, padding_value=vocab_tag['<PAD>'])\n",
    "    )\n",
    "\n",
    "\n",
    "def inference(batch):\n",
    "    #create dataset iterator to easily get text and labels encoded versions from the collate_batch.\n",
    "    train_dataloader = DataLoader(\n",
    "        batch, collate_fn=collate_batch, batch_size=params['batch_size'],\n",
    "    )\n",
    "    \n",
    "    #since our dataloader is an iterator we take our items this way. They return the output of collate_batch. Encoded\n",
    "    #versions according to vocabulary as a tensor.\n",
    "    encoded_text = next(iter(train_dataloader))[0]\n",
    "    tags = next(iter(train_dataloader))[1]\n",
    "    \n",
    "    #we can directly feed the encoded text tensor to the pre-trained model. Its shape is torch.Size([3, 1]) for 3 token\n",
    "    #sample.\n",
    "    preds = model(encoded_text)\n",
    "    \n",
    "    #here we find the most probable prediction for each token with argmax. After that we map that index to its english\n",
    "    #counterpart. This comprehension iterates 3 times since for a 3 token sample.\n",
    "    results = [idx_tag[int(i)] for i in preds.argmax(-1)]\n",
    "    return (results)\n",
    "\n",
    "eg = [[[\"John\", \"went\", \"up\"], [\"BLANK\", \"BLANK\", \"BLANK\"]]]\n",
    "print(eg[0][0])\n",
    "print(inference(eg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea1f32",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "course",
   "language": "python",
   "name": "course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
